{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe157bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from random import randrange\n",
    "    \n",
    "###Test Streaming DataLoader with PyTorch####\n",
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, filePaths, frameLength, hopSize, loadAll = False):\n",
    "            super(MyIterableDataset).__init__()\n",
    "            self.loadAll = loadAll\n",
    "            self.filePaths = self.group_recordings(filePaths)\n",
    "            self.frameLength = frameLength\n",
    "            self.hopSize = hopSize\n",
    "            self.filePage = len(self.filePaths)\n",
    "            self.filePool = list(range(self.filePage))\n",
    "            print(self.filePage)\n",
    "            random.shuffle(self.filePool)\n",
    "\n",
    "            self.currentFileIndx = 0\n",
    "            self.CurrentEEG = []\n",
    "            self.CurrentAudio = []\n",
    "            self.samplePosistions = []\n",
    "\n",
    "            self.currentSampleIndex = 0\n",
    "            self.loadDataToBuffer(self.currentFileIndx,loadAll)\n",
    "            \n",
    "            self.samplePosMap = []\n",
    "            self.generateSamplePostion()\n",
    "            \n",
    "    def group_recordings(self, files):\n",
    "        #Group recordings and corresponding stimuli.\n",
    "        new_files = []\n",
    "        grouped = itertools.groupby(sorted(files), lambda x: \"_-_\".join(x.stem.split(\"_-_\")[:3]))\n",
    "        for recording_name, feature_paths in grouped:\n",
    "            new_files += [sorted(feature_paths, key=lambda x: \"0\" if x == \"eeg\" else x)]\n",
    "        return new_files    \n",
    "    import random\n",
    "    \n",
    "    def loadDataToBuffer(self,fileIndex,loadAll):\n",
    "        if loadAll == True:\n",
    "            for i in range(self.filePage):\n",
    "                self.CurrentEEG.append(np.load(self.filePaths[self.filePool[self.currentFileIndx]][0]).astype(np.float32))\n",
    "                self.CurrentAudio.append(np.load(self.filePaths[self.filePool[self.currentFileIndx]][1]).astype(np.float32))\n",
    "        else:\n",
    "            self.CurrentEEG = np.load(self.filePaths[self.filePool[self.currentFileIndx]][0]).astype(np.float32)\n",
    "            self.CurrentAudio = np.load(self.filePaths[self.filePool[self.currentFileIndx]][1]).astype(np.float32)\n",
    "\n",
    "    \n",
    "    def generateSamplePostion(self):\n",
    "        count = 0\n",
    "        if self.loadAll == True:\n",
    "            for i in range(self.filePage):\n",
    "                totalLength,_ = self.CurrentAudio[i].shape\n",
    "                startPos = [*range(self.frameLength, totalLength+1, self.hopSize)]\n",
    "                self.samplePosMap.append(startPos)\n",
    "                noData = (totalLength-self.frameLength)//self.hopSize + 1\n",
    "                assert len(startPos)==noData\n",
    "                count += noData\n",
    "            return count\n",
    "        else:\n",
    "            for i in range(self.filePage):\n",
    "                tempAudio = np.load(self.filePaths[i][1]).astype(np.float32)\n",
    "                totalLength,_ = tempAudio.shape\n",
    "                startPos = [*range(self.frameLength, totalLength+1, self.hopSize)]\n",
    "                self.samplePosMap.append(startPos)\n",
    "                noData = (totalLength-self.frameLength)//self.hopSize + 1\n",
    "                assert len(startPos)==noData\n",
    "                count += noData\n",
    "            return count\n",
    "    def sample_random_data_number_in_one_batch(self,n, total):\n",
    "    #Return a randomly chosen list of n nonnegative integers summing to total.\n",
    "    #n: the number of total files    total: batch size\n",
    "        return [x - 1 for x in self.constrained_sum_sample_pos(n, total + n)]\n",
    "    \n",
    "    def constrained_sum_sample_pos(self,n, total):\n",
    "    #Return a randomly chosen list of n positive integers summing to total.Each such list is equally likely to occur.\"\"\"\n",
    "        dividers = sorted(random.sample(range(1, total), n - 1))\n",
    "        return [a - b for a, b in zip(dividers + [total], [0] + dividers)]\n",
    "            \n",
    "    def __iter__(self):\n",
    "       \n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.loadAll == True:\n",
    "            if self.currentSampleIndex < len(self.samplePosMap[self.filePool[self.currentFileIndx]]): # still in the same file\n",
    "                thisEnd = self.samplePosMap[self.filePool[self.currentFileIndx]][self.currentSampleIndex]\n",
    "                self.currentSampleIndex += 1\n",
    "                return self.CurrentEEG[self.filePool[self.currentFileIndx]][thisEnd-self.frameLength:thisEnd,:], self.CurrentAudio[self.filePool[self.currentFileIndx]][thisEnd-self.frameLength:thisEnd,:]\n",
    "            else: # move to the next file\n",
    "                #print(\"next file\")\n",
    "                #### need to shuffle samples from the last file\n",
    "                random.shuffle(self.samplePosMap[self.filePool[self.currentFileIndx]])\n",
    "                self.currentFileIndx +=1\n",
    "                self.currentSampleIndex = 0\n",
    "                if self.currentFileIndx < self.filePage: # still in the same iteration\n",
    "                    #self.loadDataToBuffer(self.currentFileIndx)\n",
    "                    thisEnd = self.samplePosMap[self.filePool[self.currentFileIndx]][self.currentSampleIndex]\n",
    "                    self.currentSampleIndex += 1\n",
    "                    return self.CurrentEEG[self.filePool[self.currentFileIndx]][thisEnd-self.frameLength:thisEnd,:], self.CurrentAudio[self.filePool[self.currentFileIndx]][thisEnd-self.frameLength:thisEnd,:]\n",
    "                else:\n",
    "                    #print(\"here 2\")\n",
    "                    random.shuffle(self.filePool)\n",
    "                    self.currentFileIndx = 0\n",
    "                    #self.loadDataToBuffer(self.currentFileIndx)\n",
    "                    raise StopIteration\n",
    "                    print(\"iteration done, restart\")\n",
    "        else:\n",
    "            if self.currentSampleIndex < len(self.samplePosMap[self.filePool[self.currentFileIndx]]): # still in the same file\n",
    "                thisEnd = self.samplePosMap[self.filePool[self.currentFileIndx]][self.currentSampleIndex]\n",
    "                self.currentSampleIndex += 1\n",
    "                return self.CurrentEEG[thisEnd-self.frameLength:thisEnd,:], self.CurrentAudio[thisEnd-self.frameLength:thisEnd,:]\n",
    "            else: # move to the next file\n",
    "                #print(\"next file\")\n",
    "                #### need to shuffle samples from the last file\n",
    "                random.shuffle(self.samplePosMap[self.filePool[self.currentFileIndx]])\n",
    "                self.currentFileIndx +=1\n",
    "                self.currentSampleIndex = 0\n",
    "                if self.currentFileIndx < self.filePage: # still in the same iteration\n",
    "                    self.loadDataToBuffer(self.currentFileIndx)\n",
    "                    thisEnd = self.samplePosMap[self.filePool[self.currentFileIndx]][self.currentSampleIndex]\n",
    "                    self.currentSampleIndex += 1\n",
    "                    return self.CurrentEEG[thisEnd-self.frameLength:thisEnd,:], self.CurrentAudio[thisEnd-self.frameLength:thisEnd,:]\n",
    "                else:\n",
    "                    #print(\"here 2\")\n",
    "                    random.shuffle(self.filePool)\n",
    "                    self.currentFileIndx = 0\n",
    "                    self.loadDataToBuffer(self.currentFileIndx)\n",
    "                    raise StopIteration\n",
    "                    print(\"iteration done, restart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8997b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAllLoadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, filePaths, frameLength, hopSize):\n",
    "            self.filePaths = self.group_recordings(filePaths)\n",
    "            self.frameLength = frameLength\n",
    "            self.hopSize = hopSize\n",
    "            self.filePage = len(self.filePaths)\n",
    "            # load all data to memory as continuous data\n",
    "            self.EEGData = []\n",
    "            self.AudioData = []\n",
    "            self.loadDataToBuffer()\n",
    "            \n",
    "            self.sampleIndxMap = []\n",
    "            self.noData = self.generateSamplePostion()\n",
    "            print(\"total number of training examples:\", self.noData)\n",
    "            \n",
    "    def group_recordings(self, files):\n",
    "        #Group recordings and corresponding stimuli.\n",
    "        new_files = []\n",
    "        grouped = itertools.groupby(sorted(files), lambda x: \"_-_\".join(x.stem.split(\"_-_\")[:3]))\n",
    "        for recording_name, feature_paths in grouped:\n",
    "            new_files += [sorted(feature_paths, key=lambda x: \"0\" if x == \"eeg\" else x)]\n",
    "        return new_files         \n",
    "    \n",
    "    def loadDataToBuffer(self):\n",
    "        for i in range(self.filePage):\n",
    "            self.EEGData.append(np.load(self.filePaths[i][0]).astype(np.float32))\n",
    "            self.AudioData.append(np.load(self.filePaths[i][1]).astype(np.float32))\n",
    "    \n",
    "    def generateSamplePostion(self):\n",
    "        count = 0\n",
    "        for i in range(self.filePage):\n",
    "            totalLength,_ = self.AudioData[i].shape\n",
    "            startPos = [*range(self.frameLength, totalLength+1, self.hopSize)]\n",
    "            for pos in startPos:\n",
    "                self.sampleIndxMap.append((i, pos))\n",
    "            noData = (totalLength-self.frameLength)//self.hopSize + 1\n",
    "            assert len(startPos)==noData\n",
    "            count += noData\n",
    "        return count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.noData\n",
    "    \n",
    "    def __getitem__(self, idx):      \n",
    "        return self.EEGData[self.sampleIndxMap[idx][0]][self.sampleIndxMap[idx][1]-self.frameLength:self.sampleIndxMap[idx][1],:], self.AudioData[self.sampleIndxMap[idx][0]][self.sampleIndxMap[idx][1]-self.frameLength:self.sampleIndxMap[idx][1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1feb0a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:58: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:58: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from util import rescale, find_max_epoch, print_size\n",
    "from util import training_loss, calc_diffusion_hyperparams\n",
    "\n",
    "from distributed_util import init_distributed, apply_gradient_allreduce, reduce_tensor\n",
    "\n",
    "from models2 import EEGWav_diff as WaveNet\n",
    "\n",
    "def train(window_length,hop_length, num_gpus, rank, group_name, output_directory, tensorboard_directory,\n",
    "          ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging,\n",
    "          learning_rate, batch_size_per_gpu):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    num_gpus, rank, group_name:     parameters for distributed training\n",
    "    output_directory (str):         save model checkpoints to this path\n",
    "    tensorboard_directory (str):    save tensorboard events to this path\n",
    "    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded;\n",
    "                                    automitically selects the maximum iteration if 'max' is selected\n",
    "    n_iters (int):                  number of iterations to train, default is 1M\n",
    "    iters_per_ckpt (int):           number of iterations to save checkpoint,\n",
    "                                    default is 10k, for models with residual_channel=64 this number can be larger\n",
    "    iters_per_logging (int):        number of iterations to save training log, default is 100\n",
    "    learning_rate (float):          learning rate\n",
    "    batch_size_per_gpu (int):       batchsize per gpu, default is 2 so total batchsize is 16 with 8 gpus\n",
    "    \"\"\"\n",
    "\n",
    "    # generate experiment (local) path\n",
    "    local_path = \"ch{}_T{}_betaT{}\".format(wavenet_config[\"res_channels\"],\n",
    "                                           diffusion_config[\"T\"],\n",
    "                                           diffusion_config[\"beta_T\"])\n",
    "    # Create tensorboard logger.\n",
    "    if rank == 0:\n",
    "        tb = SummaryWriter(os.path.join('exp', local_path, tensorboard_directory))\n",
    "\n",
    "    # distributed running initialization\n",
    "    if num_gpus > 1:\n",
    "        init_distributed(rank, num_gpus, group_name, **dist_config)\n",
    "\n",
    "    # Get shared output_directory ready\n",
    "    output_directory = os.path.join('exp', local_path, output_directory)\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        print(\"output directory\", output_directory, flush=True)\n",
    "\n",
    "    # map diffusion hyperparameters to gpu\n",
    "    for key in diffusion_hyperparams:\n",
    "        if key is not \"T\":\n",
    "            diffusion_hyperparams[key] = diffusion_hyperparams[key].cuda()\n",
    "\n",
    "    # predefine model\n",
    "    net = WaveNet(**wavenet_config).cuda()\n",
    "    print_size(net)\n",
    "\n",
    "    # apply gradient all reduce\n",
    "    if num_gpus > 1:\n",
    "        net = apply_gradient_allreduce(net)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # load checkpoint\n",
    "    if ckpt_iter == 'max':\n",
    "        ckpt_iter = find_max_epoch(output_directory)\n",
    "    if ckpt_iter >= 0:\n",
    "        try:\n",
    "            # load checkpoint file\n",
    "            model_path = os.path.join(output_directory, '{}.pkl'.format(ckpt_iter))\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "            # feed model dict and optimizer state\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            print('Successfully loaded model at iteration {}'.format(ckpt_iter))\n",
    "        except:\n",
    "            ckpt_iter = -1\n",
    "            print('No valid checkpoint model found, start training from initialization.')\n",
    "    else:\n",
    "        ckpt_iter = -1\n",
    "        print('No valid checkpoint model found, start training from initialization.')\n",
    "    \n",
    "    # Get the path to the config file\n",
    "    experiments_folder = \"C:/Users/YLY/Documents/eegAudChallenge/auditory-eeg-challenge-2023-code/task2_regression\"\n",
    "    task_folder = Path(experiments_folder)\n",
    "    config_path = task_folder/ \"util/config.json\"\n",
    "\n",
    "    with open(config_path) as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    data_folder = Path(config[\"dataset_folder\"])/ config[\"split_folder\"]\n",
    "    stimulus_features = [\"envelope\"]\n",
    "    features = [\"eeg\"] + stimulus_features\n",
    "\n",
    "    train_files = [path for path in Path(data_folder).resolve().glob(\"train_-_*\") if path.stem.split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "    train_dataset = CustomAllLoadDataset(train_files,window_length,hop_length)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=8,shuffle=True)\n",
    "        \n",
    "    # training\n",
    "    n_iter = ckpt_iter + 1\n",
    "    while n_iter < n_iters + 1:\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            eeg, audio = data[0].squeeze(1).cuda(), data[1].type(torch.LongTensor).cuda()\n",
    "            #print(eeg.shape,audio.shape)\n",
    "            # load audio and mel spectrogram\n",
    "            # mel_spectrogram = mel_spectrogram.cuda()\n",
    "            # audio = audio.unsqueeze(1).cuda()\n",
    "\n",
    "            # back-propagation\n",
    "            optimizer.zero_grad()\n",
    "            X = (eeg.float(), audio.float())\n",
    "            loss = training_loss(net, nn.MSELoss(), X, diffusion_hyperparams)\n",
    "            # print(loss)\n",
    "            if num_gpus > 1:\n",
    "                reduced_loss = reduce_tensor(loss.data, num_gpus).item()\n",
    "            else:\n",
    "                reduced_loss = loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # output to log\n",
    "            # note, only do this on the first gpu\n",
    "            if n_iter % iters_per_logging == 0 and rank == 0:\n",
    "                # save training loss to tensorboard\n",
    "                print(\"iteration: {} \\treduced loss: {} \\tloss: {}\".format(n_iter, reduced_loss, loss.item()))\n",
    "                tb.add_scalar(\"Log-Train-Loss\", torch.log(loss).item(), n_iter)\n",
    "                tb.add_scalar(\"Log-Train-Reduced-Loss\", np.log(reduced_loss), n_iter)\n",
    "\n",
    "            # save checkpoint\n",
    "            if n_iter > 0 and n_iter % iters_per_ckpt == 0 and rank == 0:\n",
    "                checkpoint_name = '{}.pkl'.format(n_iter)\n",
    "                torch.save({'model_state_dict': net.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict()},\n",
    "                           os.path.join(output_directory, checkpoint_name))\n",
    "                print('model at iteration %s is saved' % n_iter)\n",
    "\n",
    "            n_iter += 1\n",
    "\n",
    "    # Close TensorBoard.\n",
    "    if rank == 0:\n",
    "        tb.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c698f0dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory exp\\ch64_T100_betaT0.05\\logs/checkpoint\n",
      "EEGWav_diff Parameters: 2.191232M\n",
      "No valid checkpoint model found, start training from initialization.\n",
      "total number of training examples: 4375\n",
      "iteration: 0 \treduced loss: 0.9517078995704651 \tloss: 0.9517078995704651\n",
      "iteration: 100 \treduced loss: 0.9947216510772705 \tloss: 0.9947216510772705\n",
      "iteration: 200 \treduced loss: 1.0172399282455444 \tloss: 1.0172399282455444\n",
      "iteration: 300 \treduced loss: 0.96627277135849 \tloss: 0.96627277135849\n",
      "iteration: 400 \treduced loss: 1.016547441482544 \tloss: 1.016547441482544\n",
      "iteration: 500 \treduced loss: 1.0092779397964478 \tloss: 1.0092779397964478\n",
      "model at iteration 500 is saved\n",
      "iteration: 600 \treduced loss: 1.0026050806045532 \tloss: 1.0026050806045532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 115\u001b[0m\n\u001b[0;32m    113\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test set\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Create a dataset generator for each test subject\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m#test_files = [x for x in glob.glob(os.path.join(data_folder, \"test_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m#    json.dump(evaluation, fp)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m#logging.info(f\"Results saved at {results_path}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(window_length, hop_length, num_gpus, rank, group_name, output_directory, tensorboard_directory, ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging, learning_rate, batch_size_per_gpu)\u001b[0m\n\u001b[0;32m    123\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    124\u001b[0m X \u001b[38;5;241m=\u001b[39m (eeg\u001b[38;5;241m.\u001b[39mfloat(), audio\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m--> 125\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_hyperparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\util.py:187\u001b[0m, in \u001b[0;36mtraining_loss\u001b[1;34m(net, loss_fn, X, diffusion_hyperparams)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m#print(\"loss calc: z\", z.shape)\u001b[39;00m\n\u001b[0;32m    186\u001b[0m transformed_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(Alpha_bar[diffusion_steps]) \u001b[38;5;241m*\u001b[39m audio \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Alpha_bar[diffusion_steps]) \u001b[38;5;241m*\u001b[39m z  \u001b[38;5;66;03m# compute x_t from q(x_t|x_0)\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m epsilon_theta \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# predict \\epsilon according to \\epsilon_\\theta\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#print(\"loss calc: epsilon\", epsilon_theta.shape)\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn(epsilon_theta, z)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\models2.py:253\u001b[0m, in \u001b[0;36mEEGWav_diff.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    249\u001b[0m x \u001b[38;5;241m=\u001b[39m audio\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m#assert(x.shape[2]==1)\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m#assert(eeg.shape[2]==64)\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m#x = self.init_conv(x)\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\models2.py:215\u001b[0m, in \u001b[0;36mResidual_group.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    213\u001b[0m skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_layers):\n\u001b[1;32m--> 215\u001b[0m     h, eeg_out, skip_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meeg_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_step_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# use the output from last residual layer\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     skip \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m skip_n  \u001b[38;5;66;03m# accumulate all skip outputs\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m skip \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_layers)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\models2.py:140\u001b[0m, in \u001b[0;36mResidual_block.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    135\u001b[0m B, C, L \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m#print(\"B after\", B)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m#print(\"C after\", C)\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m#print(\"L after\", L)\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# add in diffusion step embedding\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m part_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion_step_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m part_t \u001b[38;5;241m=\u001b[39m part_t\u001b[38;5;241m.\u001b[39mview([B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_channels, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m#print(\"part_t \",part_t.shape)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\fx\\traceback.py:51\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_stack\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_overridden:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m current_stack\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Example experiment for a linear baseline method.\"\"\"\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from task2_regression.models.linear import simple_linear_model\n",
    "from task2_regression.util.dataset_generator import RegressionDataGenerator, create_tf_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "def evaluate_model(model, test_dict):\n",
    "    \"\"\"Evaluate a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: tf.keras.Model\n",
    "        Model to evaluate.\n",
    "    test_dict: dict\n",
    "        Mapping between a subject and a tf.data.Dataset containing the test\n",
    "        set for the subject.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping between a subject and the loss/evaluation score on the test set\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "    for subject, ds_test in test_dict.items():\n",
    "        logging.info(f\"Scores for subject {subject}:\")\n",
    "        results = model.evaluate(ds_test, verbose=2)\n",
    "        metrics = model.metrics_names\n",
    "        evaluation[subject] = dict(zip(metrics, results))\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    # Length of the decision window\n",
    "    window_length = 5 * 64  # 5s\n",
    "    # Hop length between two consecutive decision windows\n",
    "    hop_length = int(64*0.5) #0.5 seconds\n",
    "    epochs = 100\n",
    "    patience = 5\n",
    "    batch_size = 1\n",
    "    only_evaluate = False\n",
    "    training_log_filename = \"training_log.csv\"\n",
    "    results_filename = 'eval.json'\n",
    "\n",
    "\n",
    "    # Get the path to the config file\n",
    "    experiments_folder = \"C:/Users/YLY/Documents/eegAudChallenge/auditory-eeg-challenge-2023-code/task2_regression\"\n",
    "    task_folder = os.path.dirname(experiments_folder)\n",
    "    config_path = os.path.join(task_folder, 'util', 'config.json')\n",
    "\n",
    "    # Load the config\n",
    "    with open(\"C:/Users/YLY/Documents/eegAudChallenge/auditory-eeg-challenge-2023-code/task2_regression/util/config.json\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    # Provide the path of the dataset\n",
    "    # which is split already to train, val, test\n",
    "\n",
    "    data_folder = os.path.join(config[\"dataset_folder\"], config[\"split_folder\"])\n",
    "    stimulus_features = [\"envelope\"]\n",
    "    features = [\"eeg\"] + stimulus_features\n",
    "\n",
    "    # Create a directory to store (intermediate) results\n",
    "    results_folder = os.path.join(experiments_folder, \"results_linear_baseline\")\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "    # create a simple linear model\n",
    "    #model = simple_linear_model()\n",
    "    #model.summary()\n",
    "    #model_path = os.path.join(results_folder, \"model.h5\")\n",
    "\n",
    "    if only_evaluate:\n",
    "    #    model = tf.keras.models.load_model(model_path)\n",
    "        print(\"evaluate\")\n",
    "    else:\n",
    "\n",
    "        #train_files = [x for x in glob.glob(os.path.join(data_folder, \"train_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "        # Create list of numpy array files\n",
    "        #train_generator = RegressionDataGenerator(train_files, window_length)\n",
    "       # dataset_train = create_tf_dataset(train_generator, window_length, None, hop_length, batch_size)\n",
    "\n",
    "        # Create the generator for the validation set\n",
    "        #val_files = [x for x in glob.glob(os.path.join(data_folder, \"val_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "        #val_generator = RegressionDataGenerator(val_files, window_length)\n",
    "        #dataset_val = create_tf_dataset(val_generator, window_length, None, hop_length, batch_size)\n",
    "        # Convert the TensorFlow dataset to a PyTorch dataset\n",
    "        #train_dataset= CustomDataset(dataset_train,340585)\n",
    "        #del dataset_train\n",
    "        #val_dataset = CustomDataset(dataset_val,38357)\n",
    "       # del dataset_val \n",
    "    #### train the model\n",
    "    # Parse configs. Globals nicer in this case\n",
    "        with open('train-shallow.json') as f:\n",
    "            data = f.read()\n",
    "        config = json.loads(data)\n",
    "        train_config = config[\"train_config\"]  # training parameters\n",
    "        global dist_config\n",
    "        dist_config = config[\"dist_config\"]  # to initialize distributed training\n",
    "        global wavenet_config\n",
    "        wavenet_config = config[\"wavenet_config\"]  # to define wavenet\n",
    "        global diffusion_config\n",
    "        diffusion_config = config[\"diffusion_config\"]  # basic hyperparameters\n",
    "        global trainset_config\n",
    "        trainset_config = config[\"trainset_config\"]  # to load trainset\n",
    "        global diffusion_hyperparams\n",
    "        diffusion_hyperparams = calc_diffusion_hyperparams(\n",
    "            **diffusion_config)  # dictionary of all diffusion hyperparameters\n",
    "\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        train(window_length,hop_length, 1, 0, \"test\", **train_config)\n",
    "    \n",
    "    \n",
    "    # Evaluate the model on test set\n",
    "    # Create a dataset generator for each test subject\n",
    "    #test_files = [x for x in glob.glob(os.path.join(data_folder, \"test_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "    # Get all different subjects from the test set\n",
    "    #subjects = list(set([os.path.basename(x).split(\"_-_\")[1] for x in test_files]))\n",
    "    #datasets_test = {}\n",
    "    # Create a generator for each subject\n",
    "    #for sub in subjects:\n",
    "    #    files_test_sub = [f for f in test_files if sub in os.path.basename(f)]\n",
    "    #    test_generator = RegressionDataGenerator(files_test_sub, window_length)\n",
    "    #    datasets_test[sub] = create_tf_dataset(test_generator, window_length, None, hop_length, 1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    #evaluation = evaluate_model(model, datasets_test)\n",
    "\n",
    "    # We can save our results in a json encoded file\n",
    "    #results_path = os.path.join(results_folder, results_filename)\n",
    "    #with open(results_path, \"w\") as fp:\n",
    "    #    json.dump(evaluation, fp)\n",
    "    #logging.info(f\"Results saved at {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97daa6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804056f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
