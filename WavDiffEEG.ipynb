{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7613e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:59: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory exp\\ch64_T50_betaT0.05\\logs/checkpoint\n",
      "EEGWav_diff Parameters: 2.440832M\n",
      "No valid checkpoint model found, start training from initialization.\n",
      "3  files loaded.  4375  training examples loaded\n",
      "iteration: 0 \treduced loss: 1.007700432311086 \tloss: 1.0154560804367065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 277\u001b[0m\n\u001b[0;32m    275\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Evaluate the model on test set\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Create a dataset generator for each test subject\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# test_files = [x for x in glob.glob(os.path.join(data_folder, \"test_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m#    json.dump(evaluation, fp)\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# logging.info(f\"Results saved at {results_path}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 130\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(window_length, hop_length, num_gpus, rank, group_name, output_directory, tensorboard_directory, ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging, learning_rate, batch_size_per_gpu)\u001b[0m\n\u001b[0;32m    128\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    129\u001b[0m X \u001b[38;5;241m=\u001b[39m (eeg\u001b[38;5;241m.\u001b[39mfloat(), audio\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m--> 130\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_hyperparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\util.py:187\u001b[0m, in \u001b[0;36mtraining_loss\u001b[1;34m(net, loss_fn, X, diffusion_hyperparams)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m#print(\"loss calc: z\", z.shape)\u001b[39;00m\n\u001b[0;32m    186\u001b[0m transformed_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(Alpha_bar[diffusion_steps]) \u001b[38;5;241m*\u001b[39m audio \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m Alpha_bar[diffusion_steps]) \u001b[38;5;241m*\u001b[39m z  \u001b[38;5;66;03m# compute x_t from q(x_t|x_0)\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m epsilon_theta \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# predict \\epsilon according to \\epsilon_\\theta\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#print(\"loss calc: epsilon\", epsilon_theta.shape)\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn(epsilon_theta, z)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\models2.py:253\u001b[0m, in \u001b[0;36mEEGWav_diff.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    249\u001b[0m x \u001b[38;5;241m=\u001b[39m audio\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m#assert(x.shape[2]==1)\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m#assert(eeg.shape[2]==64)\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m#x = self.init_conv(x)\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_conv(x)\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\models2.py:215\u001b[0m, in \u001b[0;36mResidual_group.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    213\u001b[0m skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_layers):\n\u001b[1;32m--> 215\u001b[0m     h, eeg_out, skip_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meeg_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion_step_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# use the output from last residual layer\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     skip \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m skip_n  \u001b[38;5;66;03m# accumulate all skip outputs\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m skip \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_res_layers)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\eegAudChallenge\\auditory-eeg-challenge-2023-code\\models2.py:177\u001b[0m, in \u001b[0;36mResidual_block.forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    173\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_conv(out)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m#print(\"x\",x.shape)\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m#print(\"res\",res.shape)\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m#assert x.shape == res.shape\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x \u001b[38;5;241m+\u001b[39m res) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m0.5\u001b[39m), eeg_res, skip\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\site-packages\\torch\\fx\\traceback.py:57\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_stack\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\traceback.py:197\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\challenge\\lib\\linecache.py:74\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from CustomDatasetPytorch import CustomAllLoadDataset\n",
    "\n",
    "from util import rescale, find_max_epoch, print_size\n",
    "from util import training_loss, calc_diffusion_hyperparams\n",
    "\n",
    "from distributed_util import init_distributed, apply_gradient_allreduce, reduce_tensor\n",
    "\n",
    "from models2 import EEGWav_diff as WaveNet\n",
    "\n",
    "\n",
    "def train(window_length, hop_length, num_gpus, rank, group_name, output_directory, tensorboard_directory,\n",
    "          ckpt_iter, n_iters, iters_per_ckpt, iters_per_logging,\n",
    "          learning_rate, batch_size_per_gpu):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    num_gpus, rank, group_name:     parameters for distributed training\n",
    "    output_directory (str):         save model checkpoints to this path\n",
    "    tensorboard_directory (str):    save tensorboard events to this path\n",
    "    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded;\n",
    "                                    automitically selects the maximum iteration if 'max' is selected\n",
    "    n_iters (int):                  number of iterations to train, default is 1M\n",
    "    iters_per_ckpt (int):           number of iterations to save checkpoint,\n",
    "                                    default is 10k, for models with residual_channel=64 this number can be larger\n",
    "    iters_per_logging (int):        number of iterations to save training log, default is 100\n",
    "    learning_rate (float):          learning rate\n",
    "    batch_size_per_gpu (int):       batchsize per gpu, default is 2 so total batchsize is 16 with 8 gpus\n",
    "    \"\"\"\n",
    "\n",
    "    # generate experiment (local) path\n",
    "    local_path = \"ch{}_T{}_betaT{}\".format(wavenet_config[\"res_channels\"],\n",
    "                                           diffusion_config[\"T\"],\n",
    "                                           diffusion_config[\"beta_T\"])\n",
    "    # Create tensorboard logger.\n",
    "    if rank == 0:\n",
    "        tb = SummaryWriter(os.path.join('exp', local_path, tensorboard_directory))\n",
    "\n",
    "    # distributed running initialization\n",
    "    if num_gpus > 1:\n",
    "        init_distributed(rank, num_gpus, group_name, **dist_config)\n",
    "\n",
    "    # Get shared output_directory ready\n",
    "    output_directory = os.path.join('exp', local_path, output_directory)\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        print(\"output directory\", output_directory, flush=True)\n",
    "\n",
    "    # map diffusion hyperparameters to gpu\n",
    "    for key in diffusion_hyperparams:\n",
    "        if key is not \"T\":\n",
    "            diffusion_hyperparams[key] = diffusion_hyperparams[key].cuda()\n",
    "\n",
    "    # predefine model\n",
    "    net = WaveNet(**wavenet_config).cuda()\n",
    "    print_size(net)\n",
    "\n",
    "    # apply gradient all reduce\n",
    "    if num_gpus > 1:\n",
    "        net = apply_gradient_allreduce(net)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # load checkpoint\n",
    "    if ckpt_iter == 'max':\n",
    "        ckpt_iter = find_max_epoch(output_directory)\n",
    "    if ckpt_iter >= 0:\n",
    "        try:\n",
    "            # load checkpoint file\n",
    "            model_path = os.path.join(output_directory, '{}.pkl'.format(ckpt_iter))\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "            # feed model dict and optimizer state\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            if 'optimizer_state_dict' in checkpoint:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            print('Successfully loaded model at iteration {}'.format(ckpt_iter))\n",
    "        except:\n",
    "            ckpt_iter = -1\n",
    "            print('No valid checkpoint model found, start training from initialization.')\n",
    "    else:\n",
    "        ckpt_iter = -1\n",
    "        print('No valid checkpoint model found, start training from initialization.')\n",
    "\n",
    "    # Get the path to the config file\n",
    "    experiments_folder = \"C:/Users/YLY/Documents/eegAudChallenge/auditory-eeg-challenge-2023-code/task2_regression\"\n",
    "    task_folder = Path(experiments_folder)\n",
    "    config_path = task_folder / \"util/config.json\"\n",
    "\n",
    "    with open(config_path) as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    data_folder = Path(config[\"dataset_folder\"]) / config[\"split_folder\"]\n",
    "    stimulus_features = [\"envelope\"]\n",
    "    features = [\"eeg\"] + stimulus_features\n",
    "\n",
    "    train_files = [path for path in Path(data_folder).resolve().glob(\"train_-_*\") if\n",
    "                   path.stem.split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "    train_dataset = CustomAllLoadDataset(train_files, window_length, hop_length)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # training\n",
    "    n_iter = ckpt_iter + 1\n",
    "    while n_iter < n_iters + 1:\n",
    "        batch_loss = 0\n",
    "        no_epoch = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            no_epoch = i\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            eeg, audio = data[0].squeeze(1).cuda(), data[1].type(torch.LongTensor).cuda()\n",
    "            # print(eeg.shape,audio.shape)\n",
    "            # load audio and mel spectrogram\n",
    "            # mel_spectrogram = mel_spectrogram.cuda()\n",
    "            # audio = audio.unsqueeze(1).cuda()\n",
    "\n",
    "            # back-propagation\n",
    "            optimizer.zero_grad()\n",
    "            X = (eeg.float(), audio.float())\n",
    "            loss = training_loss(net, nn.MSELoss(), X, diffusion_hyperparams)\n",
    "            # print(loss)\n",
    "            if num_gpus > 1:\n",
    "                reduced_loss = reduce_tensor(loss.data, num_gpus).item()\n",
    "            else:\n",
    "                reduced_loss = loss.item()\n",
    "                batch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # output to log\n",
    "            # note, only do this on the first gpu\n",
    "        if n_iter % iters_per_logging == 0 and rank == 0:\n",
    "            # save training loss to tensorboard\n",
    "            print(\"iteration: {} \\treduced loss: {} \\tloss: {}\".format(n_iter, batch_loss / no_epoch, loss.item()))\n",
    "            tb.add_scalar(\"Log-Train-Loss\", np.log(batch_loss / no_epoch), n_iter)\n",
    "            tb.add_scalar(\"Log-Train-Reduced-Loss\", np.log(reduced_loss), n_iter)\n",
    "\n",
    "        # save checkpoint\n",
    "        if n_iter > 0 and n_iter % iters_per_ckpt == 0 and rank == 0:\n",
    "            checkpoint_name = '{}.pkl'.format(n_iter)\n",
    "            torch.save({'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict()},\n",
    "                       os.path.join(output_directory, checkpoint_name))\n",
    "            print('model at iteration %s is saved' % n_iter)\n",
    "\n",
    "        n_iter += 1\n",
    "\n",
    "    # Close TensorBoard.\n",
    "    if rank == 0:\n",
    "        tb.close()\n",
    "\n",
    "\n",
    "\"\"\"Example experiment for a linear baseline method.\"\"\"\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from task2_regression.models.linear import simple_linear_model\n",
    "from task2_regression.util.dataset_generator import RegressionDataGenerator, create_tf_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_dict):\n",
    "    \"\"\"Evaluate a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: tf.keras.Model\n",
    "        Model to evaluate.\n",
    "    test_dict: dict\n",
    "        Mapping between a subject and a tf.data.Dataset containing the test\n",
    "        set for the subject.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping between a subject and the loss/evaluation score on the test set\n",
    "    \"\"\"\n",
    "    evaluation = {}\n",
    "    for subject, ds_test in test_dict.items():\n",
    "        logging.info(f\"Scores for subject {subject}:\")\n",
    "        results = model.evaluate(ds_test, verbose=2)\n",
    "        metrics = model.metrics_names\n",
    "        evaluation[subject] = dict(zip(metrics, results))\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    # Length of the decision window\n",
    "    window_length = 5 * 64  # 5 seconds\n",
    "    # Hop length between two consecutive decision windows\n",
    "    hop_length = int(0.5 * 64)  # 0.5 seconds\n",
    "    epochs = 100\n",
    "    patience = 5\n",
    "    batch_size = 1\n",
    "    only_evaluate = False\n",
    "    training_log_filename = \"training_log.csv\"\n",
    "    results_filename = 'eval.json'\n",
    "\n",
    "    # Get the path to the config file\n",
    "    experiments_folder = \"C:/Users/YLY/Documents/eegAudChallenge/auditory-eeg-challenge-2023-code/task2_regression\"\n",
    "    task_folder = Path(experiments_folder)\n",
    "    config_path = task_folder / \"util/config.json\"\n",
    "\n",
    "    # Load the config\n",
    "    with open(config_path) as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    # Provide the path of the dataset\n",
    "    # which is split already to train, val, test\n",
    "\n",
    "    data_folder = os.path.join(config[\"dataset_folder\"], config[\"split_folder\"])\n",
    "    stimulus_features = [\"envelope\"]\n",
    "    features = [\"eeg\"] + stimulus_features\n",
    "\n",
    "    # Create a directory to store (intermediate) results\n",
    "    results_folder = os.path.join(experiments_folder, \"results_linear_baseline\")\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "    # create a simple linear model\n",
    "    # model = simple_linear_model()\n",
    "    # model.summary()\n",
    "    # model_path = os.path.join(results_folder, \"model.h5\")\n",
    "\n",
    "    if only_evaluate:\n",
    "        #    model = tf.keras.models.load_model(model_path)\n",
    "        print(\"evaluate\")\n",
    "    else:\n",
    "\n",
    "        # train_files = [x for x in glob.glob(os.path.join(data_folder, \"train_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "        # Create list of numpy array files\n",
    "        # train_generator = RegressionDataGenerator(train_files, window_length)\n",
    "        # dataset_train = create_tf_dataset(train_generator, window_length, None, hop_length, batch_size)\n",
    "\n",
    "        # Create the generator for the validation set\n",
    "        # val_files = [x for x in glob.glob(os.path.join(data_folder, \"val_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "        # val_generator = RegressionDataGenerator(val_files, window_length)\n",
    "        # dataset_val = create_tf_dataset(val_generator, window_length, None, hop_length, batch_size)\n",
    "        # Convert the TensorFlow dataset to a PyTorch dataset\n",
    "        # train_dataset= CustomDataset(dataset_train,340585)\n",
    "        # del dataset_train\n",
    "        # val_dataset = CustomDataset(dataset_val,38357)\n",
    "        # del dataset_val\n",
    "        #### train the model\n",
    "        # Parse configs. Globals nicer in this case\n",
    "        with open('train-shallow.json') as f:\n",
    "            data = f.read()\n",
    "        config = json.loads(data)\n",
    "        train_config = config[\"train_config\"]  # training parameters\n",
    "        global dist_config\n",
    "        dist_config = config[\"dist_config\"]  # to initialize distributed training\n",
    "        global wavenet_config\n",
    "        wavenet_config = config[\"wavenet_config\"]  # to define wavenet\n",
    "        global diffusion_config\n",
    "        diffusion_config = config[\"diffusion_config\"]  # basic hyperparameters\n",
    "        global trainset_config\n",
    "        trainset_config = config[\"trainset_config\"]  # to load trainset\n",
    "        global diffusion_hyperparams\n",
    "        diffusion_hyperparams = calc_diffusion_hyperparams(\n",
    "            **diffusion_config)  # dictionary of all diffusion hyperparameters\n",
    "\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        train(window_length, hop_length, 1, 0, \"test\", **train_config)\n",
    "\n",
    "    # Evaluate the model on test set\n",
    "    # Create a dataset generator for each test subject\n",
    "    # test_files = [x for x in glob.glob(os.path.join(data_folder, \"test_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "    # Get all different subjects from the test set\n",
    "    # subjects = list(set([os.path.basename(x).split(\"_-_\")[1] for x in test_files]))\n",
    "    # datasets_test = {}\n",
    "    # Create a generator for each subject\n",
    "    # for sub in subjects:\n",
    "    #    files_test_sub = [f for f in test_files if sub in os.path.basename(f)]\n",
    "    #    test_generator = RegressionDataGenerator(files_test_sub, window_length)\n",
    "    #    datasets_test[sub] = create_tf_dataset(test_generator, window_length, None, hop_length, 1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    # evaluation = evaluate_model(model, datasets_test)\n",
    "\n",
    "    # We can save our results in a json encoded file\n",
    "    # results_path = os.path.join(results_folder, results_filename)\n",
    "    # with open(results_path, \"w\") as fp:\n",
    "    #    json.dump(evaluation, fp)\n",
    "    # logging.info(f\"Results saved at {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9f2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
